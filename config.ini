; config.ini
[quantization]
load_in_4bit = true
bnb_4bit_compute_dtype = bfloat16
bnb_4bit_use_double_quant = false
bnb_4bit_quant_type = nf4
torch_dtype = float16  ; Options: float16, bfloat16, float32

; load_in_4bit = false
; bnb_4bit_compute_dtype = float32
; bnb_4bit_use_double_quant = false
; bnb_4bit_quant_type = nf4
; torch_dtype = float16  ; Options: float16, bfloat16, float32

[model]
; change this model name to the model you want to use from hugging face

; tested and working models
; meta-llama/Llama-3.1-8B-Instruct
; mistralai/Mistral-7B-Instruct-v0.3
; meta-llama/Llama-2-13b-chat-hf
; codellama/CodeLlama-34b-Instruct-hf
; Qwen/Qwen2.5-Coder-14B-Instruct
; HuggingFaceTB/SmolLM2-135M-Instruct

; this doesn't work because the model doesn't support hugging face transformers yet :(
; deepseek-ai/DeepSeek-R1-Distill-Llama-70B

name = mistralai/Mistral-7B-Instruct-v0.3

[device]
use_cuda = true

[tokens]
; stop_tokens = \n,<|endoftext|>,</s>,<eos>,<|fim_middle|>,<|fim_prefix|>,<|fim_suffix|>
stop_tokens = \n,<|endoftext|>,</s>,<eos>,[/s]

[parameters]
max_tokens = 500
temperature = 0.2
repetition_penalty = 1.4
top_k = 150
top_p = 0.8
